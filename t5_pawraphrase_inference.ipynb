{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5-pawraphrase.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "Python 3.7.9 64-bit",
      "display_name": "Python 3.7.9 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJWl3gIr1gKL"
      },
      "source": [
        "!pip3 install torch==1.4.0 -q\n",
        "!pip3 install transformers==2.9.0 -q\n",
        "!pip3 install pytorch_lightning==0.7.5 -q\n",
        "!pip3 install gdown -q"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV-j8AXOKP4A",
        "outputId": "bd4d5600-1c48-4f8d-d170-1b34b201da39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !mkdir model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6uyxYi-J8pz"
      },
      "source": [
        "import gdown \n",
        "import os.path\n",
        "\n",
        "datamodel = 'model/pytorch_model.bin'\n",
        "config_file = 'model/config.json'\n",
        "\n",
        "model_location = \"https://drive.google.com/uc?id=1-0BW2zWcI_XeT_NRT1V5p66YRCGkHs0F\"\n",
        "config_location = \"https://drive.google.com/uc?id=1-09mjTtjlgjC7bZPahBsQQoKq1ZCVCVU\"\n",
        "if os.path.isfile(datamodel):\n",
        "    ('Model was already downloaded.')\n",
        "else:\n",
        "    gdown.download(model_location, datamodel, True)\n",
        "    gdown.download(config_location, config_file, True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JNmiqR6xtRJ"
      },
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        " \n",
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('./model/')\n",
        "\n",
        "model_location = \"https://drive.google.com/uc?id=1-0BW2zWcI_XeT_NRT1V5p66YRCGkHs0F\"\n",
        "config_location = \"https://drive.google.com/uc?id=1-09mjTtjlgjC7bZPahBsQQoKq1ZCVCVU\"\n",
        "if os.path.isfile(datamodel):\n",
        "    ('Model was already downloaded.')\n",
        "    else:\n",
        "        gdown.download(model_location, datamodel, True)\n",
        "            gdown.download(config_location, config_file, True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GACBZklk1drQ",
        "outputId": "4622b8aa-129e-4e21-fec7-3c45284b6a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(\"device \", device)\n",
        "model = model.to(device)\n",
        "\n",
        "sentence = \"Trump, Biden pitch dueling visions in final sprint.\"\n",
        "\n",
        "\n",
        "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
        "\n",
        "\n",
        "max_len = 256\n",
        "\n",
        "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
        "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "beam_outputs = model.generate(\n",
        "    input_ids=input_ids, attention_mask=attention_masks,\n",
        "    do_sample=True,\n",
        "    max_length=256,\n",
        "    top_k=120,\n",
        "    top_p=0.98,\n",
        "    early_stopping=True,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print (\"\\nOriginal Phrase: \")\n",
        "print (sentence)\n",
        "print (\"\\n\")\n",
        "print (\"Paraphrased Phrase: \")\n",
        "final_outputs =[]\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    if sent.lower() != sentence.lower() and sent not in final_outputs:\n",
        "        final_outputs.append(sent)\n",
        "\n",
        "for i, final_output in enumerate(final_outputs):\n",
        "    print(\"{}: {}\".format(i, final_output))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nOriginal Phrase: \nTrump, Biden pitch dueling visions in final sprint.\n\n\nParaphrased Phrase: \n0: Trump and Biden pitch off competing visions in the final sprint.\n1: Trump and Biden make dueling visions in the final sprint.\n2: Trump and Biden pitch rivalry visions in the final sprint.\n"
          ]
        }
      ]
    }
  ]
}